<head>
  <title>
    SUSE Hack Week - Past Project
  </title>
  <link href='../../images/favicon.gif' rel='shortcut icon' />
  <link href='../../css/hackweek-single-page.css' rel='stylesheet' type='text/css' />
</head>
<body>
  <div id='navigation'>
    <li class='logo invert' id='start-link'>
      <a href='../../index.html'>
        <img border='0' height='52' src='../../images/px.gif' width='112' />
      </a>
    </li>
    <li class='invert' id='what-is-link'>
      <a href='../../index.html#what-is'>
        what is hack week?
      </a>
    </li>
    <li class='invert' id='agenda-link'>
      <a href='../../index.html#agenda'>
        agenda
      </a>
    </li>
    <li class='invert' id='projects-link'>
      <a href='../../index.html#projects'>
        projects
      </a>
    </li>
    <li class='invert' id='where-link'>
      <a href='../../index.html#where'>
        where?
      </a>
    </li>
  </div>
  <div id='content'>
    <div id='wrapper'>
      <div id='wrapper-title'>
        <h1>CONFIG_PAGE_SHIFT (aka software PAGE_SIZE)</h1>
      </div>
      <div id='wrapper-content'>
        <p>FYI: this has the "Novell Only" switch enabled.</p>
<p><b>Tags:</b> inprogress, kernel, performance, server, sles, judgeperf</p>
<h2>Description</h2>
<p>The x86 and amd64 architectures only support a fixed 4k page size. The smaller the page size, the lower amount of memory is wasted in partially unused ram, but the slower the global performance is. The next available page size is 2M which is generally too big for general purpose applications and the x86 ABI requires mmap offset paremeter to work with 4k granularity (amd64 abi fixes that problem but apps have been written for the x86 ABI so we'd rather keep supporting the 4k file offset granularity in mmap if we want to be sure not to break backwards compatibility with userland).</p>
<p>While there's nothing we can do in software to alleviate the hardware-overhead of the 4k page size (like tlb caching and frequency of the hardware pagetable walking), the 4k page size end up hurting many purely software things.</p>
<p>The xfs developers for example want to enlarge their filesystem blocksize (the filesystem blocksize has a tradeoff similar to the PAGE_SIZE, the larger the faster the filesystem but more disk space is potentially wasted), they also want to use the "normal" writeback pagecache efficient behavior when using a writable fs on top of a dvd-ram with an hardblocksize of 64k. But they can't on x86/amd64 because the PAGE_SIZE is still 4k and the whole linux kernel can't handle more than a hardblocksize of PAGE_SIZE.</p>
<p>The problem with the 4k PAGE_SIZE isn't just the maximum hardblocksize storage device we can support (i.e. dvd with 64k hardblocksize) but the whole kernel is slower because of the 4k thing. This starts from the page faults in a memcpy() that are double the number than if this was a 8k page-size, all the memory allocations (including slub/slab/blob/blam/slap/blab) are double or 4 fold or 8 fold the ones that would happen with a 8k/16k/32k page size.</p>
<p>So the whole idea is to once and for all decuple the size of the pte-entry (4k on x86/amd64) with the page allocator granularity. the HARD_PAGE_SHIFT will be 4k still, the common code PAGE_SIZE will be variable and configurable at compile time with CONFIG_PAGE_SHIFT.</p>
<p>I feel this need to happen at some point in the linux VM, since once done I can't imagine any server running with a 4k page-size anymore.</p>
<p>This requires rewriting every time we touch a pte, so this might affect drivers but all the rest of the code that isn't aware of what is a pte_t should require no change at all. There will be tons of complications like putting two (or more) pte_t arrays inside the same physical page to avoid wasting memory. But after initial skepticism because of the backwards compatibility problem, I returned overall optimistic that this is the right way to go because the objrmap/anon-vma should make it possible to alias the same 8k page on top of not naturally aligned pte entries, even for anonymous memory with mremap, by doing proper alignment math on page->offset and vma->vm_offset (pointed by the anon_vma) and shifting it right of HARD_PAGE_SHIFT. This should allow for a total backwards compatible design without any aliasing in the pagecache (only the pte won't be naturally aligned but that's ok, aliasing at the virtual level is a fundamental property of the VM and it always happens). Well, either that or I'll compete for the "Most spectacular failure" prize.</p>
<p>It's hard to imagine this done in one week but we can start at least and hopefully reach a point where we can run a benchmark to evaluate the performance boost in the best case (no idea if this doable in one week, my kernel already compiled with CONFIG_PAGE_SHIFT 13, but there's not a chance that it will boot since nothing is using HARD_PAGE_SHIFT set to 12 yet).</p>
<p>This whole issue is really a pure tradeoff between memory consumption and I/O and CPU performance (and for the dvd-ram and xfs also a way to use larger hardblocksize), so being able to benchmark is the first priority, if there's no significant benchmark gain this whole thing is unnecessary.</p>
<p>In a second step one could try to transparently map a 2M page instead of 2**9 4k pages if the vm_offset tells the alginment is ok, but I don't think we'll get there any time soon, because I expect in average production the software page size will be set to not more than 64k. 64k is probably the ideal value, only 8 times faster in allocating ram but without huge ram waste and especially with natural readahead from disk. (doing I/O in pagecache chunks larger than 64k sounds too much)</p>
<p>Alternatives: SGI is working on an alternate approach called "variable order page cache" http://lwn.net/Articles/231520/ that tries to keep the page allocator at 4k and changes the pagecache layer at order > 0 allocations. The major showstopper with their design is that there's no way they can defrag reliably the kernel memory. The memory waste will be way huge due to ram reservations and the pagecache won't be allowed to spread all over the physical ram. Worst of all the defragmenter will waste lots of cpu, so it's not a strightforward tradeoff and it has corner cases where its underperformance will be hard to evaluate (even if in the best-case the I/O performance will be good). Overall it's an inferior design that decreases the reliability of the I/O, it makes the ram less generic due to the required reservation and there's no advantage at all except being able to access devices with an hard/soft blocksize larger than 4k (it only tackle on the I/O performance with 4k fs, it hurts CPU performance if something). My design solves their troubles (I/O performance) and at the same time it boost the performance of everyone else too. It however requires compiling a kernel with a special CONFIG_PAGE_SHIFT, but then you also have to specially create xfs with a >4k blocksize so it seems a minor issue, and in theory the CONFIG_PAGE_SHIFT can also become a boottime parameter if we're ok to waste quite some cycles at runtime (the variable order page size also has to waste quite some cycles to be dynamic at runtime of course!).</p>
<h2>People</h2>
<p>Andrea Arcangeli originated this idea, already started some bit and he'll continue working on it during the Hack Week. If Nick, Andi or anybody else wants to help they're welcome. If this is turns out to be good idea, but too hard without community help, at some point we need to release it regardless of our potentially unfinished status.</p>
<p>The original idea of having a software page size larger than a hardware page size, originated at SUSE by Andrea Arcangeli and Andi Kleen while helping AMD to design their amd64 cpu, IIRC the conclusion was not to worry too much about the 4k page size being too small because we could make a soft-page-size if the time would come (or even a 2M PAGE_SIZE kernel), it's just that at the time we thought we had to break backwards compatibility (hence the ABI change in amd64 not requiring a 4k mmap offset alignment anymore), but I hope my improved/refined idea handling virtual aliasing with the page->offset/anon_vma will avoid that.</p>
<h2>Related Materials</h2>
<p>Status: hello world static binary run with init=/tmp/hello-static with the larger 8k PAGE_SIZE. This shows the filesystem and basic kernel operations are working. The pagetable handling still need to be clustered, currently it does one page fault per 4k pte, while we want to reduce the page fault rate of some order of magnitude too, not only the page-allocation-rate. The anonymous memory and pte allocations must be optimized not to take more ram than necessary. Status of the feature is still very primitive but very close to run some benchmark with init replacement to evaluate performance of fs and vm. Code is located at wotan.suse.de:~andrea/hard-page-size . Boot log was posted to the kernel@suse ML.</p>
<p>The following simple bench seems to run fine when booted as init=/tmp/bench-static after "cp -a /dev/hda /tmp/".</p>
<p>#include <stdio.h>
#include <sys/time.h>
#include <fcntl.h>
#include <unistd.h>
#include <stdlib.h>
#include <assert.h></p>
<p>#define BUFSIZE (100*1024*1024)</p>
<p>main()
{
        struct timeval before, after;
        int fd = open("/tmp/hda", O_RDONLY);
        unsigned long usec;
        char * c = malloc(BUFSIZE);
        assert(c);
        assert(fd > 2);
        for (;;) {
                gettimeofday(&before, NULL);
                if (read(fd, c, BUFSIZE) != BUFSIZE)
                        printf("errorn");
                gettimeofday(&after, NULL);
                lseek(fd, 0, 0);
                usec = (after.tv_sec - before.tv_sec)*1000000;
                usec += after.tv_usec - before.tv_usec;
                printf("%d usecn", usec);
        }
}</p>
<p>virtualization shows very skewed timings, and my amd64 test box isn't functional at the moment due to problems in my server room so I couldn't really measure anything yet :(</p>
      </div>
    </div>
  </div>
</body>
